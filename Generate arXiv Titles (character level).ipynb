{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> RNN-based Generation of arXiv Titles at Character Level </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get data from arXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/ArXiv): \"arXiv (pronounced 'archive') is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online.\"\n",
    "\n",
    "The code below uses the package [arxivpy](https://github.com/titipata/arxivpy), which is a wrapper for the [arXiv API](https://arxiv.org/help/api/index).\n",
    "\n",
    "We use it to fetch titles, categories, and published dates from paper submissions in the 'cs' (Computer Science) and 'stats' (Statistics) categories, in the following sub-categories:\n",
    "\n",
    "- cs.CV: Computer Vision and Pattern Recognition\n",
    "- cs.CL: Computation and Language\n",
    "- cs.LG: Learning\n",
    "- cs.AI: Artificial Intelligence\n",
    "- cs.NE: Neural and Evolutionary Computing\n",
    "- stat.ML: Machine Learning\n",
    "\n",
    "#### Important:\n",
    "There is no need to run the cell below, as the dataset is already provided in the 'data' folder. It takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arxivpy\n",
    "# import pandas as pd\n",
    "\n",
    "# category_list = ['cs.CV', 'cs.CL', 'cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']\n",
    "\n",
    "# titles = []\n",
    "# terms1 = []\n",
    "# terms2 = []\n",
    "# dates = []\n",
    "\n",
    "# for category in category_list:\n",
    "#     search_query = arxivpy.generate_query(terms=category, prefix='category')\n",
    "#     articles = arxivpy.query(search_query, start_index=0, max_index=5000, results_per_iteration=1000, wait_time=5.0, sort_by='lastUpdatedDate')\n",
    "#     for i in range(len(articles)):\n",
    "#         titles.append(articles[i]['title'])\n",
    "#         terms1.append(articles[i]['term'])\n",
    "#         terms2.append(articles[i]['terms'])\n",
    "#         dates.append(articles[i]['publish_date'])\n",
    "\n",
    "# dataset = pd.DataFrame({'title': titles, 'term':terms1, 'terms':terms2, 'publish_date':dates})\n",
    "# dataset.to_csv('data/arxiv_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upgrade CNTK to the latest version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default CNTK version for the Python runtimes in Azure Notebooks is 2.0 (when this notebook was created). The CNTK model in this notebook was created and trained with CNTK 2.4. Therefore it is necessary to upgrade CNTK due to compatibility issues between CNTK versions. Here we upgrade it to the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: cntk in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (2.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-deps cntk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "print(C.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare arXiv data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the provided dataset and build a list with all unique titles available that will be used for the model training. We also preprocess the data by removing any title that has some character that is not an ASCII letter, not a digit, not a punctuation character, or not a blank space.\n",
    "\n",
    "Then we pad each title by preceeding it with a 'Start of Sentence' token, given by the character '>' and appending an 'End of Sentence' token, given by the '\\n' character. These special tokens will be used to facilitate the definition of input and output data for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles in the training dataset: 36380\n",
      "Some examples of sentences in the training data:\n",
      ">recurrent batch normalization\n",
      "\n",
      ">smooth loss functions for deep top-k classification\n",
      "\n",
      ">simnets: a generalization of convolutional networks\n",
      "\n",
      ">databright: towards a global exchange for decentralized data ownership and trusted computation\n",
      "\n",
      ">distribution-dependent concentration inequalities for tighter generalization bounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/arxiv_titles.csv', engine='python')\n",
    "data = data.drop_duplicates().reset_index()\n",
    "\n",
    "titles = data['title']\n",
    "allowed_chars = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "processed_titles = []\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    invalids = [char for char in titles[i] if char not in allowed_chars]\n",
    "    if len(invalids) == 0:\n",
    "        title = re.sub(' +', ' ', titles[i])\n",
    "        processed_titles.append(title.lower())\n",
    "\n",
    "data_train = ['>' + title + '\\n' for title in processed_titles]\n",
    "np.random.shuffle(data_train)\n",
    "\n",
    "print('Number of titles in the training dataset: %d' % len(data_train))\n",
    "\n",
    "print('Some examples of sentences in the training data:')\n",
    "for title in data_train[0:5]:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary and lookup dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first create a list containing each unique character from all titles available in the dataset.\n",
    "\n",
    "From this list we then create 2 dictionaries:\n",
    "One mapping from each character in the vocabulary to a numeric index, and the other mapping from each numeric index to the corresponding character in the vocabulary.\n",
    "\n",
    "These dictionaries will help to build the one-hot encoded representation of the data and also to map form the numeric outputs from the model back to the corrsponding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average title length: 70\n",
      "Max title length: 219\n",
      "Min title length: 8\n",
      "Vocabulary Length: 70\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(''.join(data_train)))\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "char_to_index = { ch:i for i,ch in enumerate(sorted(vocab)) }\n",
    "index_to_char = { i:ch for i,ch in enumerate(sorted(vocab)) }\n",
    "\n",
    "avg_title_len = int(np.mean([len(x) for x in data_train]))\n",
    "max_title_len = int(np.max([len(x) for x in data_train]))\n",
    "min_title_len = int(np.min([len(x) for x in data_train]))\n",
    "\n",
    "print('Average title length: %d' % avg_title_len)\n",
    "print('Max title length: %d' % max_title_len)\n",
    "print('Min title length: %d' % min_title_len)\n",
    "print('Vocabulary Length: %d' % vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare batch data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNTK has a high level API for efficient reading and feeding data for model training, which are very useful when you have data that doesn't fit into memory and it is then necessary to stream it from external storage into your training session. This API also takes care of efficient random sampling and batching, but has a proprietary data format. More details [here](https://cntk.ai/pythondocs/Manual_How_to_feed_data.html).\n",
    "\n",
    "But in several situations, as this one, your data is already in a tabular format (such as a CSV file) and is small enough to be fitted in memory. In this case you have to explicitly prepare your training data, usually into mini-batches, as described here.\n",
    "\n",
    "Now we prepare the data that will be fed into the neural network during model training. We define 2 sets of data: one for model input, which begins with '>' tokens followed by the arXiv titles from the dataset, and the other given by the same arXiv titles followed by '\\n' tokens. In this way, we build inputs and outputs to be fed into the neural network allowing it to learn to predict the next token in a sequence, as illustrted by the following diagram:\n",
    "\n",
    "<img src=\"./figures/fig4.png\" alt=\"RNN for predicting the next token\" style=\"width: 600px;\"/>\n",
    "\n",
    "First we need to map from characters to the corresponding numeric representation. Here we first map each character to its numeric vocabulary index. We then convert this index to the corresponding one-hot encoded vector.\n",
    "\n",
    "Finally, we divide the data into mini-batches of 64 sequences each. Here each sequence corresponds to a given arXiv title from the dataset that will be unrolled for 'Backpropagation Through Time' during training.\n",
    "\n",
    "In CNTK we can just define each minibatch as a list of NumPy arrays. Notice that these NumPy arrays can be of variable sizes, corresponding to the variable lenghts we have for the arXiv titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of minibatches: 569\n",
      "Shapes of the first 5 elements in the first minibatch of data_x:\n",
      "(30, 70)\n",
      "(52, 70)\n",
      "(52, 70)\n",
      "(95, 70)\n",
      "(84, 70)\n"
     ]
    }
   ],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "for seq in data_train:\n",
    "    index_x = [char_to_index[ch] for ch in ''.join(seq)][:-1]\n",
    "    index_y = index_x[1:] + [char_to_index['\\n']]\n",
    "    index_x = np.eye(vocab_len, dtype=np.float32)[index_x]\n",
    "    index_y = np.eye(vocab_len, dtype=np.float32)[index_y]\n",
    "    data_x.append(index_x)\n",
    "    data_y.append(index_y)\n",
    "\n",
    "mb_size = 64 # number of sequences in each mini batch\n",
    "data_len = len(data_x)\n",
    "data_x = [data_x[s : s+mb_size] for s in range(0, data_len, mb_size)]\n",
    "data_y = [data_y[s : s+mb_size] for s in range(0, data_len, mb_size)]\n",
    "\n",
    "num_mb = len(data_x)\n",
    "print('Number of minibatches: %d' % num_mb)\n",
    "\n",
    "print('Shapes of the first 5 elements in the first minibatch of data_x:')\n",
    "for i in range(5):\n",
    "    print(data_x[0][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input and output variables, and the network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network we build has the architecture shown in the following diagram:\n",
    "\n",
    "<img src=\"./figures/fig5.png\" alt=\"RNN model architecture\" style=\"width: 600px;\"/>\n",
    "\n",
    "It takes as input a vector of one-hot encoded characters corresponding to a given arXiv title from the dataset. This one-hot encoded vectors go to a Recurrent Neural Network block, which is comprised of a Stabilization Layer, a Normalization Layer, and a GRU-based one-directional Recurrent Layer. The output of this Recurrent Neural Network block is then fed into a Dense Layer.\n",
    "\n",
    "Notice that the Recurrent Neural Network block is constructed in a way that we can define the stacking of multiple such blocks, using the CNTK *For()* function.\n",
    "\n",
    "Notice also that it is a common pattern in CNTK (and also in other Deep Learning frameworks) to define the last Dense Layer without an activation function when performing multinomial classification. The activation (Softmax in this case) is defined later, together with the model Cost (Loss) Function.\n",
    "\n",
    "The placeholder for the input values is defined by the variable *X*. The variable *Y* defines the placeholder for the oputput values, which are used to \"teach the network\" the true labels in a supervised manner during training. Remember that in our setup a true label is defined as the next token in a sequence.\n",
    "\n",
    "The actual RNN model is defined by the *create_model()* function. CNTK's [Layer library](https://docs.microsoft.com/en-us/python/cognitive-toolkit/layerref?view=cntk-py-2.5.1) allows you to build models in a pattern known as [Function Composition](https://en.wikipedia.org/wiki/Function_composition).\n",
    "\n",
    "In this way we instantiate our RNN model just by calling the *create_model()* function. The returned object from this function is also a function which we finally use to pass the input variable *X* to the model. The model predicted output is defined by the variable *Z*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = C.sequence.input_variable(shape=(vocab_len))\n",
    "Y = C.sequence.input_variable(shape=(vocab_len))\n",
    "\n",
    "n_hidden = 1024\n",
    "n_layers = 1\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0):\n",
    "        model = C.layers.Sequential([\n",
    "                C.layers.For(range(n_layers), lambda:\n",
    "                    C.layers.Sequential([C.layers.Stabilizer(),\n",
    "                                         C.layers.LayerNormalization(),\n",
    "                                         C.layers.Recurrence(C.layers.GRU(shape=n_hidden))])),\n",
    "                C.layers.Dense(vocab_len)])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "Z = model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the error, cost function, learner and trainer objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the CNTK objects needed for model training:\n",
    "\n",
    "The model error, which is expressed as a classification error. We model the prediction of the next token as a classification problem, so given an input token the next token in the sequence is predicted at the output as one of the possible tokens in the vocabulary.\n",
    "\n",
    "The model [cost function](https://en.wikipedia.org/wiki/Loss_function) (also known as the loss function), which is defined as the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss with [softmax](https://en.wikipedia.org/wiki/Softmax_function) (standard for classification problems).\n",
    "\n",
    "The [learning rate](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Background) for the optimization algorithm, which is defined in a scheduled manner. This allows the specification of a decreasing learning rate as the optimization progresses.\n",
    "\n",
    "The [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum), which is used by some optimization algorithms. In this case, it is also defined in a scheduled manner allowing for the momentum decaying as the optimization progresses.\n",
    "\n",
    "The gradient clipping threshold, used to help avoiding the exploding gradient problem with RNNs.\n",
    "\n",
    "The learner, which defines the optimization algorithm to be used during training. Here we use the [adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) optimization algorithm.\n",
    "\n",
    "The progress_printer, which is an optional helper object that allows you to print learning statistics during model training.\n",
    "\n",
    "And finally the trainer object, which wraps the model predicted output *Z*, the *cost* function, the model *error*, the *learner*, and optionally the *progress_printer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 3435731 parameters in 9 parameter tensors.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "error = C.classification_error(Z, Y)\n",
    "\n",
    "cost = C.cross_entropy_with_softmax(Z, Y)\n",
    "\n",
    "lr = C.learning_parameter_schedule([0.0005 * mb_size]*10 +\n",
    "                                   [0.0001 * mb_size]*10 +\n",
    "                                   [0.00005 * mb_size]*10 +\n",
    "                                   [0.00001 * mb_size],\n",
    "                                   minibatch_size=mb_size,\n",
    "                                   epoch_size=num_mb*mb_size*avg_title_len)\n",
    "\n",
    "m = C.momentum_schedule(0.999, minibatch_size=mb_size)\n",
    "\n",
    "gc = 5.0\n",
    "\n",
    "learner = C.adam(parameters=Z.parameters, lr=lr, momentum=m,\n",
    "                 gradient_clipping_threshold_per_sample=gc,\n",
    "                 gradient_clipping_with_truncation=True)\n",
    "\n",
    "progress_printer = C.logging.ProgressPrinter(freq=num_mb, tag='Training')\n",
    "\n",
    "trainer = C.Trainer(Z, (cost, error), learner, progress_printer)\n",
    "\n",
    "print(C.logging.log_number_of_parameters(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to sample from the model at inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function below to sample tokens, which are characters in this case, from the model output. After the model learns to predict the next token in a sequence, we can sample an entire sequence that looks very similar to sequences learned by the model.\n",
    "\n",
    "The sampling process is represented in the following diagram:\n",
    "\n",
    "<img src=\"./figures/fig6.png\" alt=\"Sampling from an RNN model\" style=\"width: 700px;\"/>\n",
    "\n",
    "We start by feeding the '>' token and a hidden state initialized as a vector of zeros to the network. Then, we sample from the softmax output according to one of the following strategies:\n",
    "\n",
    "- argmax: we just get the maximum value from the model output as the vocabulary index for the sampled token.\n",
    "- softmax: we apply the softmax function to the model output and optionally rescale it using the value from the *temp* parameter, a value between 0 and 1 whare a value of 0 means sampling from the vocabulary at random with equal probability, and a value of 1 means sampling from the vocabulary according to the softmax distribution.\n",
    "\n",
    "We then feed the sampled token and the previous hidden state to the model and sampled another token. We keep doing this until we sample an '\\n' token.\n",
    "\n",
    "Notice that a *temp* parameter between 0 and 1 has the effect of smoothing the softmax distribution which in turn causes the sampling of sequences that differ more from the learned distribution.\n",
    "\n",
    "A note about the *mask* argument: CNTK allows you to also pass a mask argument together with the input and output values for training or inference. This mask controls if you are passing a complete sequence (mask = True), or if your sequence continues (mask = False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n_chars, n_titles, seed='>', temp=1, use_argmax=False):\n",
    "    output = []\n",
    "    \n",
    "    for i in range(n_titles):\n",
    "        \n",
    "        chars = []\n",
    "        x = np.zeros((1, vocab_len), dtype=np.float32)\n",
    "        idx = char_to_index[seed]\n",
    "        x[0, idx] = 1 \n",
    "            \n",
    "        counter = 0\n",
    "        eos = char_to_index['\\n']\n",
    "        if seed != '>':\n",
    "            chars.append(seed)\n",
    "        \n",
    "        mask = [True]\n",
    "        while (counter <= n_chars):\n",
    "            arguments = ({X : [x]}, mask)\n",
    "            mask = [False]\n",
    "            p = Z.eval(arguments)[0][0]\n",
    "            if use_argmax:\n",
    "                p = C.argmax(p).eval()\n",
    "                idx = int(p)\n",
    "            else:\n",
    "                p = C.softmax(p).eval()\n",
    "                p = np.power(p, (temp))\n",
    "                p = p / np.sum(p)\n",
    "                idx = np.random.choice(range(vocab_len), p=p.ravel())\n",
    "            c = index_to_char[idx]\n",
    "            chars.append(c)\n",
    "            x = np.zeros((1, vocab_len), dtype=np.float32)\n",
    "            x[0, idx] = 1\n",
    "                \n",
    "            if idx == eos:\n",
    "                counter = n_chars+1\n",
    "            counter += 1\n",
    "        \n",
    "        if (chars[len(chars) - 1] != '\\n'):\n",
    "                chars.append('\\n')\n",
    "        output.append(''.join(chars))\n",
    "\n",
    "    return(''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNTK also has a high level API for model training, which is described [here](https://cntk.ai/pythondocs/Manual_How_to_train_using_declarative_and_imperative_API.html). It allows you to define a training session object that takes care of the training loop and is very useful specially when training in a distributed fashion.\n",
    "\n",
    "Another option, which we use here, is to explicitly control the training loop.\n",
    "\n",
    "In this setup we define an outer loop that iterates over the entire training data. Each of these iterations is also known as an *epoch*. We also define an inner loop that iterates over the mini-batches. For each mini-batch, we feed the input and output values to the network in the *train_minibatch()* method of the *trainer* object. Finally we compute the loss for each mini-batch and for the entire epoch. For this model we also compute the [perplexity](https://en.wikipedia.org/wiki/Perplexity), which is a common evaluation metric for language models.\n",
    "\n",
    "#### Important:\n",
    "There is no need to run the cell below, as a trained model is already provided in the 'models' folder. It takes a long time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costs = []\n",
    "# perps = []\n",
    "# log = open('logs/generate_arxiv_char_log.txt' , 'a')\n",
    "\n",
    "# iter = 100\n",
    "\n",
    "# start_time = time.asctime()\n",
    "# print('Start training time: ' + start_time)\n",
    "# log.write('Start training time: ' + start_time + '\\n\\n')\n",
    "\n",
    "# for i in range(iter):\n",
    "    \n",
    "#     print('Iteration: %i' % (i))\n",
    "#     log.write('Iteration: %i' % (i) + '\\n')\n",
    "    \n",
    "#     epoch_cost = 0\n",
    "#     epoch_perp = 0\n",
    "#     for k in range(num_mb):\n",
    "#         mb_X, mb_Y = data_x[k], data_y[k]\n",
    "#         masks = [True] * len(mb_X)\n",
    "#         arguments = ({X : mb_X, Y : mb_Y}, masks)\n",
    "#         trainer.train_minibatch(arguments)\n",
    "#         minibatch_cost = trainer.previous_minibatch_loss_average\n",
    "#         minibatch_error = trainer.previous_minibatch_evaluation_average\n",
    "#         epoch_cost += minibatch_cost / num_mb\n",
    "#         epoch_perp += np.exp(minibatch_cost) / num_mb\n",
    "    \n",
    "#     print(\"Cost after itaration %i: %f\" % (i, epoch_cost))\n",
    "#     log.write(\"Cost after itaration %i: %f\" % (i, epoch_cost) + '\\n')\n",
    "#     costs.append(epoch_cost)\n",
    "    \n",
    "#     print('Perplexity after iteration %i: %f' % (i, epoch_perp))\n",
    "#     log.write(\"Perplexity after itaration %i: %f\" % (i, epoch_perp) + '\\n')\n",
    "#     perps.append(epoch_perp)\n",
    "    \n",
    "#     print('Sampling 5 titles from the model:\\n')\n",
    "#     log.write('Sampling 5 titles from the model:\\n\\n')\n",
    "#     s = sample(max_title_len, 5, '>') + '\\n\\n'\n",
    "#     print(s)\n",
    "#     log.write(s)\n",
    "    \n",
    "# end_time = time.asctime()\n",
    "# print('End training time: ' + end_time)\n",
    "# log.write('End training time: ' + end_time + '\\n')\n",
    "    \n",
    "# model_file = 'models/generate_arxiv_char_epoch_%d.dnn'  % (i+1)\n",
    "# trainer.save_checkpoint(model_file)\n",
    "    \n",
    "# log.close()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(costs)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(perps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some titles from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sample some titles from the model. We will sample 100 titles, with at most the maximum title lenght found in the training data, and using a value of 1 for the temperature parameter.\n",
    "\n",
    "We then print the sampled titles that are not in the training data.\n",
    "\n",
    "Before running the cell below you need to run all but the commented cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated titles: 100\n",
      "Number of generated titles not in the training data: 100\n",
      "Examples of some generated titles not in the training data:\n",
      "\n",
      "activation regisel structure inference for diabetecting human-robot teams and mecons in iketse substrates\n",
      "\n",
      "an extremal framework for drone squares\n",
      "\n",
      "inhoduce-based deep ensemble reconstruction from road networks using monolinear dynamical recurrent two-stip method\n",
      "\n",
      "learning multimodal word representation via pairwise spatial recurrent neural networks\n",
      "\n",
      "reinforcement learning based visual saliency and geodesic distance estimation using gaussian mixture relevance\n",
      "\n",
      "face-frizud, pseudo predictive modeling and latent factor analysis\n",
      "\n",
      "learning limited memory based on clinical and quantitative inverses using context-aware valuation\n",
      "\n",
      "unification of evolutionary algorithm work loop\n",
      "\n",
      "exchangeable random minim: a simple youttunebcornel approach\n",
      "\n",
      "stock truncation in a nonparametric model for the future\n",
      "\n",
      "universal reinforcement learning algorithms: surface and negative sampling problem\n",
      "\n",
      "demand frame and shrinkage edit: a new reinforcement on facial keypoints and control\n",
      "\n",
      "learning to reinforcement learning in neighborhood functions\n",
      "\n",
      "deeply-basadapting radiagrams with neural provably access time likelihood\n",
      "\n",
      "the role of multilayer perceptron networks\n",
      "\n",
      "stochastic single-shot deep learning for word-co-occurrence statistics\n",
      "\n",
      "framework for topic modeling using support vector machine for rule-based smt\n",
      "\n",
      "automatic skin lesion segmentation using richer transformation by half-informaticle selection\n",
      "\n",
      "long-term visually grounded learning with noisy based multi-objective evolutionary algorithm\n",
      "\n",
      "bayesian modeling of multiple dynamic networks with a factor-oriented dialog\n",
      "\n",
      "on the use of human ethical records by minimising\n",
      "\n",
      "statistical inference using lyzicalogy: an algorithm\n",
      "\n",
      "fast binary compression of neural circuits\n",
      "\n",
      "deep reinforcement learning for instructed predictive mixing models\n",
      "\n",
      "emotional backdoors to technological images of centes via wearable fava: the human interaction proper\n",
      "\n",
      "likelihood-free inference by global training for image sentence selection\n",
      "\n",
      "using pattern based target recognition techniques for function approximation\n",
      "\n",
      "expressivity of tidee: inferring the number of conditions\n",
      "\n",
      "risk scoring based on mixture-of-fact semantic mapping patterns\n",
      "\n",
      "exploiting diversity for natural language parsing: the upper and lidard problem\n",
      "\n",
      "directional palocus recommendation system: a review based on extrasolution reinforcement learning channel\n",
      "\n",
      "extracting noise-based features over wide selection using stein variation by augmented spiking neural network\n",
      "\n",
      "programming for uncertainty management with deep relu networks\n",
      "\n",
      "different algorithms for fake news detection from observational data\n",
      "\n",
      "the acquisition of a neural network\n",
      "\n",
      "online monocular slotting system for the periva classification ?\n",
      "\n",
      "an ontology for bayesian random similarity endoscopis\n",
      "\n",
      "learning similarity-based word pac-background constraints in data threating\n",
      "\n",
      "axiomatizing classifier definitions of self organizing maps for approximate coursestigenes\n",
      "\n",
      "are features extraction methods based on avoidance without metaheuristics\n",
      "\n",
      "revisiting the 16th dependent competition \"in systems theories\n",
      "\n",
      "margins, irspect based computer visibility\n",
      "\n",
      "the complexity of risk from motion\n",
      "\n",
      "sequence modeling with distributed word representations\n",
      "\n",
      "a comparison between different linear optimization for efficient face attribute images\n",
      "\n",
      "weighting statistical machine translation by graph search: an examination for them all cham\n",
      "\n",
      "median-based exploration with many experimental descriptions\n",
      "\n",
      "bayesian nonparametric models for dependency grammar\n",
      "\n",
      "detecting off-to-image-to-image learners\n",
      "\n",
      "aligning scheme based on vector quality detection\n",
      "\n",
      "an online decision-making algorithm for nlps dataset analysis\n",
      "\n",
      "exploring dialogical query logic and polish\n",
      "\n",
      "audio event and scene recognition: a survey\n",
      "\n",
      "property-driven statistical machine translation for block framework\n",
      "\n",
      "pose guided style transfer with varying data with sparse causal models\n",
      "\n",
      "a bilingual local search optimization as a measure of deep qris marketplaces using multiple view theory\n",
      "\n",
      "k-means algorithms to improve the contribution of events, human, and memera image restoration\n",
      "\n",
      "psychological and non-linear modularing estimation\n",
      "\n",
      "do deep neural networks react for reinforcement learning with gradients\n",
      "\n",
      "universal particle mdeldal chaining network for abstract argumentation\n",
      "\n",
      "feasibility of photo-domain mip decomposition from a soft mt (uls)mes\n",
      "\n",
      "comparison among the opinions of fmri analysis: ising models and medians with extension to novel objects\n",
      "\n",
      "the majority of lens diamrowice challenge\n",
      "\n",
      "a growing log-linear box programming approach to semantic part model-adaptive measurements\n",
      "\n",
      "recognition of ising models by national covariance culture\n",
      "\n",
      "real-time video highlights for back-propagation\n",
      "\n",
      "modeling browser-based neural machine translation\n",
      "\n",
      "roles of convolution neural networks\n",
      "\n",
      "a probabilistic analysis of belief network inference from a single vaex\n",
      "\n",
      "jointly learning to align and converted point clouds from large scale data: mapropdins\n",
      "\n",
      "structured sparse convolutional neural networks for perturbation modeling\n",
      "\n",
      "textually cdns review on computer vision\n",
      "\n",
      "context-scening index bounds by means of the game-of-conditional generative adversarial network\n",
      "\n",
      "massively parallel adversarial autoencoders\n",
      "\n",
      "high-quality accelerated stochastic maximum entropy models\n",
      "\n",
      "learning concept drift\n",
      "\n",
      "revisiting reflections on hole is not at hor, wrosht\n",
      "\n",
      "semi-parametric network structure learning\n",
      "\n",
      "learning multi-timedial and demm framework for face recognition via neural network approach\n",
      "\n",
      "cohe entity tubing by aidsometic assockator contrology\n",
      "\n",
      "information-theoretic limits for data clustering on graphs\n",
      "\n",
      "learning pre-trained networks\n",
      "\n",
      "attention networks for feature scale flow\n",
      "\n",
      "multi-person pose estimation and soft prediction problems using structural information fusion\n",
      "\n",
      "on sensor function adaptation for translation effort estimation\n",
      "\n",
      "training recurrent networks for brain evaluation of multiple lightness\n",
      "\n",
      "dimensionality reduction in deep networks: methods and applications\n",
      "\n",
      "representation learning strategies for neural machine translation\n",
      "\n",
      "multi-stage regional model for polarity recurrent neural networks\n",
      "\n",
      "energy-efficient bayesian learning with a general framework for sparse mind activation function\n",
      "\n",
      "memery of multi-attribute properties for rari speech pattern discovery in mymp\n",
      "\n",
      "understanding humans' scripts (qual) varier, and dynamic real-time reschedbiai\n",
      "\n",
      "towards a two-tread bandwidth extraction tool\n",
      "\n",
      "multiple-smoothing for new development evaluation selection\n",
      "\n",
      "deep learning for spoken language understanding\n",
      "\n",
      "exact learning: structure and, organisation observed by positive kernels\n",
      "\n",
      "weakly supervised conjugate gradient for the indus discovery in systematic reviews\n",
      "\n",
      "semi-supervised learning of structured prediction: consistency and accuracy for context forests\n",
      "\n",
      "improving human gaze estimation for automated projection of reactive well-presearch at semeval-2017 task i\n",
      "\n",
      "efficient convolutional neural network for embodied trift\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.restore_from_checkpoint('models/generate_arxiv_char_epoch_100.dnn')\n",
    "Z = trainer.model\n",
    "\n",
    "generated_titles = sample(max_title_len, 100, '>', temp=1).split('\\n')\n",
    "generated_titles = ['>' + title + '\\n' for title in generated_titles[:-1]]\n",
    "print('Number of generated titles: %d' % len(generated_titles))\n",
    "\n",
    "generated_titles = list(set(generated_titles) - set(generated_titles).intersection(set([d[0] for d in data_train])))\n",
    "print('Number of generated titles not in the training data: %d' % len(generated_titles))\n",
    "\n",
    "print('Examples of some generated titles not in the training data:\\n')\n",
    "titles = [w[1:-1] for w in generated_titles]\n",
    "for title in titles[0:100]:\n",
    "    print(title + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
